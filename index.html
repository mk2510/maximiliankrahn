<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Rui Li</title>
  
  <meta name="author" content="Maximilian Krahn">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="assets/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- Bio -->
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rui Li</name>
              </p>
              <p>
                <!-- at NPWU supervised by Prof. <a href="https://scholar.google.com.hk/citations?view_op=list_works&hl=en&hl=en&user=-wzlS7QAAAAJ"> Yanning Zhang</a>. I was -->
                I am a Ph.D. student at NWPU and was a visiting student at <a href="https://vision.ee.ethz.ch/"> the Computer Vision Laboratory (CVL)</a> of ETH ZÃ¼rich, working with Prof. <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html"> Luc Van Gool </a> and Dr. <a href="https://federicotombari.github.io/"> Federico Tombari </a>. Prior to that, I had an internship at DJI Technology working with Dr. <a href="https://donggong1.github.io/"> Dong Gong</a> and Dr. <a href="https://yvanyin.net/">Wei Yin</a>. 
                I have a broad interest in 3D computer vision, including monocular depth estimation, multi-view stereo and 3D neural implicit representations. 
                <br>
                <br>
                I am working on integrating traditional geometric principles with advanced deep learning methodologies for <i>unconstrained</i> 3D reconstruction and scene understanding. Feel free to drop me an <a href="mailto:lirui.david@gmail.com">email</a> if you are interested.
              </p>
              <p style="text-align:center">
                <a href="mailto:lirui.david@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=wIv44N4AAAAJ">Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ruili3">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/leedaray">Twitter(X)</a>
              </p>
            </td>
            <td style="padding: 10% 2% 10% 2%;width:40%;max-width:40%">
              <img src="assets/ruili.jpg" width="200" alt="photo">
            </td>
          </tr>

        </tbody></table>
        


        <!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <ul>
              <!-- href="https://ruili3.github.io/kyn" -->
              <li style="margin-bottom: 10px;"> [2024.04.01] <a href="https://ruili3.github.io/kyn/">KYN</a> (VL-guided single-view 3D reconstruction) is accepted to CVPR 2024!</li>

              <!-- href="https://github.com/Wuuu3511/GoMVS" -->
              <li style="margin-bottom: 10px;"> [2024.03.30] <a href="https://wuuu3511.github.io/gomvs/">GoMVS</a> (<span style="color: red;"> ranks 1<sup>st</sup> </span> on <a href="https://www.tanksandtemples.org/leaderboard/AdvancedF/">Tank & Temples Advanced set</a>) is accepted to CVPR 2024!</li>
            
              <li>[2023.04.18] The <a href="https://github.com/ruili3/dynamic-multiframe-depth">code</a> and <a href="https://arxiv.org/abs/2304.08993">paper</a> of the <a href="https://ruili3.github.io/dymultidepth/index.html">DyMultiDepth</a> (CVPR 2023) has been released! </li>
            </ul>
          </td>
          </tr>
        </tbody></table>



        <!-- Research -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- KYN -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/kyn.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.03658">
                <papertitle>Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</papertitle>
              </a>
              <br>
              <strong>Rui Li</strong>,
              <a href="https://tobiasfshr.github.io/">Tobias Fischer</a>,
              <a href="https://mattiasegu.github.io/">Mattia Segu</a>,
              <a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a>,
              <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>,
              <a href="https://federicotombari.github.io/">Federico Tombari</a>
              <br>
              <i> Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 </i>
              <br>
              <a href="./kyn/index.html">project page</a>
              /
              <a href="https://arxiv.org/abs/2404.03658">arXiv</a>
              /
              <a href="https://github.com/ruili3/Know-Your-Neighbors">code</a>

              <p></p>
              <p>
                A single-view 3D reconstruction method that disambiguates occluded scene geometry by utilizing Vision-Language semantics and spatial reasoning.
              </p>
            </td>
          </tr>

          <!-- GoMVS -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/gcmvsnet.jpg' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.07992">
                <papertitle>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</papertitle>
              </a>
              <br>
              <a>Jiang Wu*</a>,
              <strong>Rui Li*</strong>,
              <a href="https://haofeixu.github.io/">Haofei Xu</a>,
              <a>Wenxun Zhao</a>,
              <a>Yu Zhu</a>,
              <a>Jinqiu Sun</a>,
              <a>Yanning Zhang</a>
              (* equal contribution)
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 
              <br>
              <a href="https://wuuu3511.github.io/gomvs/">project page</a>
              /
              <a href="https://arxiv.org/abs/2404.07992">arXiv</a>
              /
              <a href="https://github.com/Wuuu3511/GoMVS">code</a>
              <p></p>
              <p>
                A multi-view stereo approach with geometrically consistent matching cost aggregation using monocular normals. <br>
                <font color="red"><strong>1<sup>st</sup> place</strong></font> on <a href="https://www.tanksandtemples.org/leaderboard/AdvancedF/">Tanks and Temples (Advanced) leaderboard</a>.
              </p>
            </td>
          </tr>


          <!-- DyMultiDepth -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/dymultidepth.png' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.08993">
                <papertitle>Learning to Fuse Monocular and Multi-view Cues for Multi-frame Depth Estimation in Dynamic Scenes</papertitle>
              </a>
              <br>
              <strong>Rui Li</strong>,
              <a href="https://donggong1.github.io/index.html">Dong Gong</a>,
              <a href="https://yvanyin.net/">Wei Yin</a>,
              <a href="https://stan-haochen.github.io/">Hao Chen</a>,
              Yu Zhu,
              Kaixuan Wang,
              Xiaozhi Chen,
              Jinqiu Sun,
              Yanning Zhang
              <br>
              <i> Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023 </i>
              <br>
              <a href="./dymultidepth/index.html">project page</a>
              /
              <a href="https://www.youtube.com/watch?v=0ViYXt2bpuM">video</a>
              /
              <a href="https://arxiv.org/abs/2304.08993">arXiv</a>
              /
              <a href="https://github.com/ruili3/dynamic-multiframe-depth">code</a>

              <p></p>
              <p>
                A multi-frame depth estimation approach that handles the dynamic areas by fusing monocular and multi-view cues in a mask-free manner. 
              </p>
            </td>
          </tr>


          <!-- SemDepth -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/semdepth.jpg' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320322007762">
                <papertitle>Learning depth via leveraging semantics: Self-supervised monocular depth estimation with both implicit and explicit semantic guidance</papertitle>
              </a>
              <br>
              <strong>Rui Li</strong>,
              Danna Xue,
              Shaolin Su,
              Xiantuo He,
              Qing Mao,
              Yu Zhu,
              Jinqiu Sun,
              Yanning Zhang
              <br>
              <i> Pattern Recognition (<strong>PR</strong>)</em>, 2023 </i>
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0031320322007762">paper</a>
              /
              <!-- <a href=""></a> -->
              code (coming soon)

              <p></p>
              <p>
                A semantic-guided self-supervised depth estimation method that conducts implicit/explicit semantic guidance for high-quality and sharp depth.
              </p>
            </td>
          </tr>


          <!-- Enhancing -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='assets/enhancing.jpg' width="180">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413706">
                <papertitle>Enhancing Self-supervised Monocular Depth Estimation via Incorporating Robust Constraints</papertitle>
              </a>
              <br>
              <strong>Rui Li</strong>,
              Xiantuo He,
              Yu Zhu,
              Xianjun Li,
              Jinqiu Sun,
              Yanning Zhang
              <br>
              <i> ACM International Conference on Multimedia (<strong>ACM MM</strong>)</em>, 2020 </i>
              <br>

              <p></p>
              <p>
                A self-supervised depth estimation method that incorporates robust constraints to improve photometric supervision.
              </p>
            </td>
          </tr>

        <!-- Academic Services -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding-top:5px;padding-bottom:5px;padding-left:20px;padding-right:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <ul>
              <li>Conference Reviewer</li>
              <ul>
                <li>CVPR: 2023, 2024</li>
                <li>ECCV: 2022, 2024</li>
                <li>ICCV: 2023</li>
              </ul>  
            </ul>
          </td>
          </tr>
        </tbody></table>




        <!-- footnote -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <a href="https://github.com/jonbarron/website"><font size="2" color="lightgray">awesome website template</font></a>
              <br>
            </font>
            </p>
            </td>
          </tr>
          </table>

      </td>
    </tr>
  </table>

  




</body>

</html>
